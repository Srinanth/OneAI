The user starts by choosing an AI model and providing their own API key. Each chat is treated as a separate conversation, with its own artifact and token tracking. For every message, the selected model simultaneously returns a response for the user and an updated structured artifact containing key facts, decisions, open questions, and assumptions. The app tracks token usage per chat and per model in real time. If a chat approaches its API limit, the system automatically generates a “last artifact” via the current model or a lightweight fallback summarizer, preserving context. Users can start new chats with the same or different models, creating independent artifacts for each conversation. When switching models within a chat, the latest artifact is passed as the starting context so the new model can continue seamlessly. Users can view or edit artifacts at any time, keeping control and transparency. This design allows multiple parallel conversations, manages token limits efficiently, and ensures continuity across model switches and API restrictions.